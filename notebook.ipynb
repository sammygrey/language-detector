{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detector for Programatic Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all used packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import utils\n",
    "\n",
    "import os, PIL\n",
    "from glob import glob\n",
    "\n",
    "# Regular Expression Parsing\n",
    "import re\n",
    "\n",
    "# Natural Language Toolkit\n",
    "import nltk; nltk.download(\"stopwords\"); nltk.download(\"wordnet\")\n",
    "\n",
    "# Language Token Processing and Frequency Distribution Calculator\n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "\n",
    "# Generalized Machine/Deep Learning Codependencies\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Subsets of imported libraries for easier use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model Architecture\n",
    "Sequential = tf.keras.models.Sequential\n",
    "\n",
    "# Connective Layers with Dropout\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "# Early Stopping Optimization\n",
    "EarlyStopping = tf.keras.callbacks.EarlyStopping\n",
    "\n",
    "# Natural Text-Based Language Processing Layers with RNN\n",
    "Embedding = tf.keras.layers.Embedding\n",
    "LSTM = tf.keras.layers.LSTM\n",
    "SpatialDropout1D = tf.keras.layers.SpatialDropout1D\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPool2D = tf.keras.layers.MaxPool2D\n",
    "\n",
    "# Language Tokenization Filter\n",
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "# Padding Function for Dataset Ingestion Preprocessing\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the dataset from file and create a dataframe with categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>corpus</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>braswell</td>\n",
       "      <td>// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * C...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eigensolver_complex</td>\n",
       "      <td>// This file is part of Eigen, a lightweight C...</td>\n",
       "      <td>c++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bootstrap</td>\n",
       "      <td>#define USE_UPDATE_CHECKS\\nusing System;\\nusin...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common</td>\n",
       "      <td>// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * c...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>homo_sm4_decrypt</td>\n",
       "      <td>#include &lt;iostream&gt;\\n\\n#include &lt;helib/helib.h...</td>\n",
       "      <td>c++</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                                             corpus  \\\n",
       "0             braswell  // SPDX-License-Identifier: GPL-2.0+\\n/*\\n * C...   \n",
       "1  eigensolver_complex  // This file is part of Eigen, a lightweight C...   \n",
       "2            Bootstrap  #define USE_UPDATE_CHECKS\\nusing System;\\nusin...   \n",
       "3               common  // SPDX-License-Identifier: GPL-2.0+\\n/*\\n * c...   \n",
       "4     homo_sm4_decrypt  #include <iostream>\\n\\n#include <helib/helib.h...   \n",
       "\n",
       "  language  \n",
       "0        c  \n",
       "1      c++  \n",
       "2       c#  \n",
       "3        c  \n",
       "4      c++  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title, corpus, type\n",
    "column_names = [\"title\", \"corpus\", \"language\"]\n",
    "c_list = os.listdir('dataset/c')\n",
    "cpp_list = os.listdir('dataset/c++')\n",
    "cs_list = os.listdir('dataset/c#')\n",
    "\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "for (c,cpp,cs) in zip(c_list, cpp_list, cs_list):\n",
    "    if c:\n",
    "        with open(f'dataset/c/{c}', encoding=\"utf8\") as file:\n",
    "            dict = {'title': c.split('.')[0], 'corpus': file.read(), 'language': 'c'}\n",
    "            df = df.append(dict, ignore_index = True)\n",
    "            file.close()\n",
    "    if cpp:\n",
    "        with open(f'dataset/c++/{cpp}', encoding=\"utf8\") as file:\n",
    "            dict = {'title': cpp.split('.')[0], 'corpus': file.read(), 'language': 'c++'}\n",
    "            df = df.append(dict, ignore_index = True)\n",
    "            file.close()\n",
    "    if cs:\n",
    "        with open(f'dataset/c#/{cs}', encoding=\"utf8\") as file:\n",
    "            dict = {'title': cs.split('.')[0], 'corpus': file.read(), 'language': 'c#'}\n",
    "            df = df.append(dict, ignore_index = True)\n",
    "            file.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus      0\n",
       "language    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_check = [\"corpus\", \"language\"]\n",
    "processed = df[features_to_check]\n",
    "processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that easily allows us to remove stopwards from our bodies of text aswell as generally cleaning it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(data, stopwords, dtype=\"frame\", feature=\"corpus\"):\n",
    "  \"\"\" Function to remove special characters, digits, stop words, \n",
    "  unimportant symbols, and other unnecessary noise from our dataset. \"\"\"\n",
    "  if dtype == \"frame\":\n",
    "    data[feature] = data[feature].apply(\n",
    "        lambda corpus: \" \".join(corpus.lower() for corpus in corpus.split())\n",
    "    )\n",
    "    data[feature] = data[feature].str.replace(\n",
    "        \"\\d+\", \"\"\n",
    "    )\n",
    "    data[feature] = data[feature].apply(\n",
    "        lambda token: \" \".join(token for token in token.split() if token not in stopwords)\n",
    "    )\n",
    "    data[feature] = data[feature].apply(\n",
    "        lambda token: \" \".join([Word(token).lemmatize() for token in token.split()])\n",
    "    )\n",
    "  elif dtype == \"list\":\n",
    "    data = [\" \".join(corpus.lower() for corpus in corpus.split()) for corpus in data]\n",
    "    data = [re.sub(\"\\d+\", \"\", corpus) for corpus in data]\n",
    "    data = [\" \".join(token for token in token.split() if token not in stopwords) for token in data]\n",
    "    data = [\" \".join([Word(token).lemmatize() for token in token.split()]) for token in data]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of stopwords specific to our dataset and purpose aswell as cleaning our data by removing the provided stopwards and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sammy\\AppData\\Local\\Temp/ipykernel_2568/3940087954.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = data[feature].apply(\n",
      "C:\\Users\\Sammy\\AppData\\Local\\Temp/ipykernel_2568/3940087954.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[feature] = data[feature].str.replace(\n",
      "C:\\Users\\Sammy\\AppData\\Local\\Temp/ipykernel_2568/3940087954.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = data[feature].str.replace(\n",
      "C:\\Users\\Sammy\\AppData\\Local\\Temp/ipykernel_2568/3940087954.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = data[feature].apply(\n",
      "C:\\Users\\Sammy\\AppData\\Local\\Temp/ipykernel_2568/3940087954.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = data[feature].apply(\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"for\", \"while\", \"do\", \"goto\", \"if\", \"else\", \"{\", \"}\", \"\\\\n\", \"i\", \"n\", \"//\",\"/*\",\"*\"] #coding specific stopwords\n",
    "\n",
    "processed = clean_corpus(data=processed, \n",
    "                         stopwords=stopwords,\n",
    "                         dtype=\"frame\",\n",
    "                         feature=\"corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the head of the data to see a difference in bodies of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spdx-license-identifier: gpl-.+ copyright (c) ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this file is part of eigen, a lightweight c++ ...</td>\n",
       "      <td>c++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#define use_update_checks using system; using ...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              corpus language\n",
       "0  spdx-license-identifier: gpl-.+ copyright (c) ...        c\n",
       "1  this file is part of eigen, a lightweight c++ ...      c++\n",
       "2  #define use_update_checks using system; using ...       c#"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer to help us change our words into numerical values that the computer can interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(tokenizer, data):\n",
    "  \"\"\" Function to tokenize input data for model training/testing. \"\"\"\n",
    "  return pad_sequences(tokenizer.texts_to_sequences(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tokenizer (setting max words per document to 500) and create an X variable using our preprocessed code text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=500, split=\" \")\n",
    "tokenizer.fit_on_texts(processed[\"corpus\"].values)\n",
    "\n",
    "X = tokenize_dataset(tokenizer, processed[\"corpus\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Y variable to use in the testing against our model and split X and Y into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3271)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(processed['language'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7, \n",
    "                                                    test_size=0.3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layers for use in our RNN, this will make it look a lot nicer when adding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer for Token-Specific Vectorization\n",
    "input_embedding_layer = Embedding(500, 120, input_length=X.shape[1])\n",
    "\n",
    "# Dropout Regularizer for Text Embedding\n",
    "embedding_dropout_layer = SpatialDropout1D(0.4)\n",
    "\n",
    "# First Recurrent LSTM Cellular Architecture\n",
    "first_recurrent_layer = LSTM(176, \n",
    "                             dropout=0.2, \n",
    "                             recurrent_dropout=0.2, \n",
    "                             return_sequences=True)\n",
    "\n",
    "# Second Recurrent LSTM Cellular Architecture\n",
    "second_recurrent_layer = LSTM(176, \n",
    "                              dropout=0.2, \n",
    "                              recurrent_dropout=0.2)\n",
    "\n",
    "# Final Dense Layer for Output Extraction\n",
    "output_connective_layer = Dense(3, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model and add our layers into it and get a summary of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 3271, 120)         60000     \n",
      "                                                                 \n",
      " spatial_dropout1d_7 (Spatia  (None, 3271, 120)        0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 3271, 176)         209088    \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 176)               248512    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3)                 531       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 518,131\n",
      "Trainable params: 518,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model Architecture Design\n",
    "model = Sequential()\n",
    "\n",
    "# Add All Initialized Layers in Effective Sequence\n",
    "model.add(input_embedding_layer)\n",
    "model.add(embedding_dropout_layer)\n",
    "model.add(first_recurrent_layer)\n",
    "model.add(second_recurrent_layer)\n",
    "model.add(output_connective_layer)\n",
    "\n",
    "# Get Model Summary for Confirmation\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile our model using categorical crossentropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model with Specified Loss and Optimization Functions\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up early stopping and define the batch size and epochs we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Early Stopping Callback Optimizer\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "\n",
    "# Define Batch Size and Epochs as Hyperparameters\n",
    "batch_size, epochs = 32, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit our training values to our model and allow for verbose output so we can see its progress over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 103s 103s/step - loss: 1.0978 - accuracy: 0.3810\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 127s 127s/step - loss: 1.0870 - accuracy: 0.5714\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 138s 138s/step - loss: 1.0742 - accuracy: 0.5714\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 134s 134s/step - loss: 1.0549 - accuracy: 0.5714\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 142s 142s/step - loss: 1.0194 - accuracy: 0.5238\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 151s 151s/step - loss: 0.9158 - accuracy: 0.6190\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 156s 156s/step - loss: 0.7241 - accuracy: 0.6190\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 160s 160s/step - loss: 0.7760 - accuracy: 0.5238\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 160s 160s/step - loss: 0.6912 - accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 159s 159s/step - loss: 0.4863 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit Learning Model Using Training Data and Configured Hyperparameters\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[callback],\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of 1.000 - this is overtrained to the dataset or undersampled\n",
    "With an easy dataset with only a few features an accuracy close to that of 100% is to be expected but not in a dataset with hundreds of features. \n",
    "It is likely that because this dataset is so restricted by its size it is overtrained. \n",
    "While 10-11 epochs is generally the training models the selection of data we are utilizing is too small to gain 100% of the information regarding data that lies outside of what is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 1.6684 - accuracy: 0.5556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6683546304702759, 0.5555555820465088]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Learned Model Using Testing Data\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the accuracy of the model is not 100%. It would make sense that despite training accuracy being 100% at the end of epoch 10, the testing accuracy is closer to 50%. While the data may have been over trained it is still successful enough to classify around 55% of our code as its correct language. This is better than a blind model which would have an average accuracy of about 33.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Our Predicted Labels\n",
    "y_pred = pd.DataFrame(data=model.predict(X_test))\n",
    "y_pred = y_pred.apply(round, axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a confusion matrix for further testing of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2],\n",
       "       [0, 2, 2],\n",
       "       [0, 0, 3]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmat = confusion_matrix(y_true=y_test.values.argmax(axis=1), \n",
    "                        y_pred=y_pred.values.argmax(axis=1))\n",
    "cmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our confusion matrix we can see the results of our testing:\n",
    "- The two C programs we passed in were evaluated as c# programs\n",
    "    - This may be due to some C# programs being modular in nature as they are for Unity and do not have many of the object oriented things you would expect to see in a regular C# script\n",
    "- Two C++ programs were evaluated correctly while two were evaluated as C#\n",
    "    - Both C++ and C# are similar in nature and this is an easy mistake for the model to make. \n",
    "- All three C# programs tested were correctly identified\n",
    "\n",
    "Our model seems to lean heavily towards evaluating models as being written in the C# language. \n",
    "This is likely due to a combination of overtraining and the small sample size we have in our dataset. \n",
    "All that being said the model is more successful than a blind pick for the languages being only 33% accurate while ours is 55.5%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8b1b78dffae98ab8aece9bd1a597e812d51b074407e1f3c96d6a2b5d9235a4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
